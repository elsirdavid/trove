{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Weakly Supervised Named Entity Recognition (NER)\n",
    "\n",
    "We'll use the public [BioCreative V Chemical Disease Relation](https://biocreative.bioinformatics.udel.edu/tasks/biocreative-v/track-3-cdr/) (BC5CDR) dataset, focusing on Chemical entities. \n",
    "\n",
    "See `../applications/BC5CDR/` for the complete labeling function set used in our paper. \n",
    "\n",
    "## Installation Instructions\n",
    "\n",
    "- Trove requires access to the [Unified Medical Language System (UMLS)](https://www.nlm.nih.gov/research/umls/licensedcontent/umlsknowledgesources.html) which is freely available after signing up for an account with the National Library of Medicine. Visit the link above and download the latest \"UMLS Metathesaurus Files\" release [2020AB](https://download.nlm.nih.gov/umls/kss/2020AB/umls-2020AB-metathesaurus.zip) and run our UMLS install script. \n",
    "- Unzip the preprocessed BioCreative V CDR chemical dataset `bc5cdr.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'../../trove')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Unlabeled Data & Define Entity Classes\n",
    "\n",
    "### A. Load Preprocessed Documents\n",
    "This notebook assumes documents have already been preprocessed for sentence boundary detection and dumped into JSON format. See `preprocessing/README.md` for details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged Entities: 5203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization Error: Token is not a head token Annotation[Chemical](Cl|1240-1242) 19692487\n",
      "Tokenization Error: Token is not a head token Annotation[Chemical](Cl|1579-1581) 15075188\n",
      "Errors: Span Alignment: 2/5347 (0.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged Entities: 5345\n",
      "Tagged Entities: 5385\n",
      "CPU times: user 27.7 s, sys: 525 ms, total: 28.2 s\n",
      "Wall time: 29.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import transformers\n",
    "from trove.dataloaders import load_json_dataset\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "\n",
    "data_dir = \"data/bc5cdr/\"\n",
    "dataset = {\n",
    "    split : load_json_dataset(f'{data_dir}/{split}.cdr.chemical.json', tokenizer)\n",
    "    for split in ['train', 'dev', 'test']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### B. Define Entity Categories\n",
    "In popular biomedical annotators such as [NCBO BioPortal](https://bioportal.bioontology.org/annotator), we configure the annotator by selecting a set of semantic categories which define our entity class and a corresponding set of ontologies mapped to those types.  \n",
    "\n",
    "Trove uses a similar style of interface in API form. For `CHEMICAL` tagging, we define an entity class consisting of [UMLS Semantic Network](https://semanticnetwork.nlm.nih.gov/) types mapped to $\\{0,1\\}$. The semantic network defines 127 concept categories called _Semantic Types_ (e.g., Disease or Syndrome , Medical Device) which are mappable to 15 coarser-grained _Semantic Groups_ (e.g., Anatomy, Chemicals & Drugs, Disorders). \n",
    "\n",
    "We use the _Chemicals & Drugs_ (CHEM) semantic group as the basis of our positive class label $1$, removing some categories (e.g., Gene or Genome) that do not match the definition of chemical as outlined in the BC5CDR annotation guidelines. Non-chemical STYs define our negative class label $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the chemical entity definition\n",
    "entity_def = pd.read_csv('data/chemical_semantic_types.tsv', sep='\\t')\n",
    "class_map = {row.TUI:row.LABEL for row in entity_def.itertuples() if row.LABEL != -1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Ontology Labeling Sources\n",
    "### A. Unified Medical Language System (UMLS) Metathesaurus\n",
    "The UMLS Metathesaurus is a convenient source for deriving labels, since it provides over 200 source vocabularies (terminologies) with consistent entity categorization provided by the UMLS Semantic Network.\n",
    "\n",
    "The first time this is run, Trove requires access to the installation zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 86 µs, sys: 417 µs, total: 503 µs\n",
      "Wall time: 835 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from trove.labelers.umls import UMLS\n",
    "\n",
    "# initialize UMLS\n",
    "backend = 'pandas'\n",
    "if not UMLS.is_initalized(backend=backend):\n",
    "    print(f'Please initalize the UMLS before running this notebook. See `umls_install.sh`')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply some minimal preprocessing to each source vocabularies term set, as outlined in the Trove paper. The most important settings are:\n",
    "- `SmartLowercase()`, a string matching heuristic for preserving likely abbreviations and acronyms\n",
    "- `min_char_len`, `filter_rgx`, filters for terms that are single characters or numbers  \n",
    "\n",
    "Other choices are largely for speed purposes, such as restricting the max token length used for string matching. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 21s, sys: 5.69 s, total: 1min 27s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from trove.labelers.umls import UMLS\n",
    "from trove.transforms import SmartLowercase\n",
    "\n",
    "# english stopwords\n",
    "stopwords = set(open('data/stopwords.txt','r').read().splitlines())\n",
    "stopwords = stopwords.union(set([t[0].upper() + t[1:] for t in stopwords]))\n",
    "\n",
    "# options for filtering terms\n",
    "config = {\n",
    "    \"type_mapping\"  : \"TUI\",  # TUI = semantic types, CUI = concept ids\n",
    "    'min_char_len'  : 2,\n",
    "    'max_tok_len'   : 8,\n",
    "    'min_dict_size' : 500,\n",
    "    'stopwords'     : stopwords,\n",
    "    'transforms'    : [SmartLowercase()],\n",
    "    'languages'     : {\"ENG\"},\n",
    "    'filter_sabs'   : {\"SNOMEDCT_VET\"},\n",
    "    'filter_rgx'    : r'''^[-+]*[0-9]+([.][0-9]+)*$'''  # filter numbers\n",
    "}\n",
    "\n",
    "umls = UMLS(backend=backend, **config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 2s, sys: 1.4 s, total: 1min 3s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "def map_entity_classes(dictionary, class_map):\n",
    "    \"\"\"\n",
    "    Given a dictionary, create the term entity class probabilities\n",
    "    \"\"\"\n",
    "    k = len([y for y in set(class_map.values()) if y != -1])\n",
    "    ontology = {}\n",
    "    for term in dictionary:\n",
    "        proba = np.zeros(shape=k).astype(np.float32)\n",
    "        for cls in dictionary[term]:\n",
    "            # ignore abstains\n",
    "            idx = class_map[cls] if cls in class_map else -1\n",
    "            if idx != -1:\n",
    "                proba[idx - 1] += 1\n",
    "        # don't include terms that don't map to any classes\n",
    "        if np.sum(proba) > 0:\n",
    "            ontology[term] = proba / np.sum(proba)\n",
    "    return ontology\n",
    "\n",
    "# These are the top 10 ontologies as ranked by term overlap with the BC5CDR training set\n",
    "terminologies = ['CHV', 'SNOMEDCT_US', 'NCI', 'MSH']\n",
    "\n",
    "ontologies = {\n",
    "    sab : map_entity_classes(umls.terminologies[sab], class_map)\n",
    "    for sab in terminologies\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.3 s, sys: 234 ms, total: 15.6 s\n",
      "Wall time: 15.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create dictionaries for our Schwartz-Hearst abbreviation detection labelers\n",
    "positive, negative = set(), set()\n",
    "\n",
    "for sab in umls.terminologies:\n",
    "    for term in umls.terminologies[sab]:\n",
    "        for tui in umls.terminologies[sab][term]:\n",
    "            if tui in class_map and class_map[tui] == 1:\n",
    "                positive.add(term)\n",
    "            elif tui in class_map and class_map[tui] == 0:\n",
    "                negative.add(term)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Additional Ontologies: ChEBI Database\n",
    "We also want to utilize non-UMLS ontologies. External databases such as ChEBI or CTD typically don't include rich mappings to Semantic Network types, so we treat this as an ontology/dictionary mapping to a single class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ontologies import ChebiDatabase\n",
    "\n",
    "config = {\n",
    "    'min_char_len'  : 2,\n",
    "    'max_tok_len'   : 8,\n",
    "    'min_dict_size' : 1,\n",
    "    'stopwords'     : stopwords,\n",
    "    'transforms'    : [SmartLowercase()],\n",
    "    'languages'     : None,\n",
    "    'filter_sources': None,\n",
    "    'filter_rgx'    : r'''^[-+]*[0-9]+([.][0-9]+)*$'''  # filter numbers\n",
    "}\n",
    "chebi = ChebiDatabase(cache_path=None, **config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. ADAM Biomedical Abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Sequence Labeling Functions\n",
    "### A. Guideline Labeling Functions\n",
    "\n",
    "Annotation guidelines -- the instructions provided to domain experts when labeling training data -- can have a big impact on the generalizability of named enity classifiers. These instructions include seeminly simple choices such as whether to include determiners in entity spans (\"the XXX\") or more complex tagging choices like not labeling negated mentions of drugs. These choices are baked into the dataset and expensive to change. \n",
    "\n",
    "With weak supervision, many of these annotation assumptions can encoded as labeling functions, making training set changes faster, more flexible, and lower cost. For our `Chemical` labeling functions, we use the instructions provided [here](https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf) (pages 5-6) to create small dictionaries encoding some of these guidelines. Note that these can be easily expanded on, and in some cases complex rules (e.g., not annotating polypeptides with more than 15 amino acids) can be coupled with richer structured resources to create more sophisticated rules. \n",
    "\n",
    "We also fine it useful to include labeling functions that exclude numbers and punctuation tokens, another common flag in online biomedical annotators. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trove.labelers.labeling import (\n",
    "    OntologyLabelingFunction,\n",
    "    DictionaryLabelingFunction, \n",
    "    RegexEachLabelingFunction\n",
    ")\n",
    "\n",
    "max_ngrams = 8\n",
    "\n",
    "# load our guideline dictionaries\n",
    "df = pd.read_csv('data/bc5cdr_guidelines.tsv', sep='\\t',)\n",
    "guidelines = {\n",
    "    t:np.array([1.,0.]) if y==1 else np.array([0.,1.]) \n",
    "    for t,y in zip(df.TERM, df.LABEL)\n",
    "}\n",
    "\n",
    "# use guideline negative examples as an additional stopword list\n",
    "guideline_stopwords = {t:2 for t in df[df.LABEL==0].TERM}\n",
    "stopwords = {t:2 for t in stopwords}\n",
    "\n",
    "guideline_lfs = [\n",
    "    OntologyLabelingFunction('guidelines', guidelines, max_ngrams=max_ngrams),\n",
    "    DictionaryLabelingFunction('stopwords', stopwords, 2),\n",
    "    DictionaryLabelingFunction('punctuation', set('!\"#$%&*+,./:;<=>?@[\\\\]^_`{|}~'), 2),\n",
    "    RegexEachLabelingFunction('numbers', [r'''^[-]*[1-9]+[0-9]*([.][0-9]+)*$'''], 2)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Semantic Type Labeling Functions\n",
    "\n",
    "The bulk of our supervision comes from structured medical ontologies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.7 s, sys: 226 ms, total: 22.9 s\n",
      "Wall time: 22.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from trove.labelers.abbreviations import SchwartzHearstLabelingFunction\n",
    "\n",
    "ontology_lfs = [\n",
    "    OntologyLabelingFunction(\n",
    "        f'UMLS_{name}', \n",
    "        ontologies[name], \n",
    "        max_ngrams=max_ngrams, \n",
    "        stopwords=guideline_stopwords \n",
    "    )\n",
    "    for name in ontologies\n",
    "]\n",
    "\n",
    "ontology_lfs += [\n",
    "    SchwartzHearstLabelingFunction('UMLS_schwartz_hearst_1', positive, 1, stopwords=guideline_stopwords),\n",
    "    SchwartzHearstLabelingFunction('UMLS_schwartz_hearst_2', negative, 2)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_ontology_lfs = [\n",
    "    DictionaryLabelingFunction('chebi', chebi.terms(), 1, max_ngrams=max_ngrams, stopwords=guideline_stopwords)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. SynSet Labeling Functions\n",
    "\n",
    "For biomedical concepts, abbreviations and acronymns (more generally \"short forms\") are a large source of ambiguity. \n",
    "These can be ambiguous to human readers as well, so authors of PubMed abstract typically define ambiguous terms when they are introduced in text. We can take adavantage of this redundancy to both handle ambiguous mentions and identify out-of-ontology short forms using classic text mining techniques such as the [Schwartz-Hearst algorithm](https://psb.stanford.edu/psb-online/proceedings/psb03/schwartz.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# TBD\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Task-specific Labeling Functions\n",
    "\n",
    "Ontology-based labeling functions can do suprisingly well on their own, but we can get more performance gains by adding custom labeling functions. For this demo, we focus on simple rules that are easy to create via data exploration but any existing rule-based model can be transformed into a labeling function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from trove.labelers.labeling import RegexLabelingFunction\n",
    "\n",
    "task_specific_lfs = []\n",
    "\n",
    "# We noticed parentheses were causing errors so this labeling function \n",
    "# identifies negative examples, e.g. (n=100), (10%)\n",
    "parens_rgxs = [\n",
    "    r'''[(](p|n)\\s*([><=]+|(less|great)(er)*)|(ml|mg|kg|g|(year|day|month)[s]*)[)]|[(][0-9]+[%][)]'''\n",
    "]\n",
    "# case insensitive \n",
    "parens_rgxs = [re.compile(rgx, re.I) for rgx in parens_rgxs]\n",
    "task_specific_lfs.append(RegexLabelingFunction('LF_parentheses', parens_rgxs, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs = guideline_lfs + ontology_lfs + ext_ontology_lfs #+ task_specific_lfs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Construct the Label Matrix $\\Lambda$\n",
    "### A. Apply Sequence Labeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel(n_jobs=4)\n",
      "auto block size=3495\n",
      "Partitioned into 4 blocks, [3494 3495] sizes\n",
      "CPU times: user 22.8 s, sys: 5.99 s, total: 28.8 s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import itertools\n",
    "from trove.labelers.core import SequenceLabelingServer\n",
    "\n",
    "X_sents = [\n",
    "    dataset['train'].sentences,\n",
    "    dataset['dev'].sentences,\n",
    "    dataset['test'].sentences,\n",
    "]\n",
    "\n",
    "labeler = SequenceLabelingServer(num_workers=4)\n",
    "L_sents = labeler.apply(lfs, X_sents)\n",
    "\n",
    "\n",
    "# Wall time: 1min 48s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "splits = ['train', 'dev', 'test']\n",
    "tag2idx = {'O':2, 'I-Chemical':1}\n",
    "\n",
    "X_words = [\n",
    "    np.array(list(itertools.chain.from_iterable([s.words for s in X_sents[i]]))) \n",
    "    for i,name in enumerate(splits)\n",
    "]\n",
    "\n",
    "X_seq_lens = [\n",
    "    np.array([len(s.words) for s in X_sents[i]])\n",
    "    for i,name in enumerate(splits)\n",
    "]\n",
    "\n",
    "X_doc_seq_lens = [  \n",
    "    np.array([len(doc.sentences) for doc in dataset[name].documents]) \n",
    "    for i,name in enumerate(splits)\n",
    "]\n",
    "\n",
    "Y_words = [\n",
    "    [dataset['train'].tagged(i)[-1] for i in range(len(dataset['train']))],\n",
    "    [dataset['dev'].tagged(i)[-1] for i in range(len(dataset['dev']))],\n",
    "    [dataset['test'].tagged(i)[-1] for i in range(len(dataset['test']))],\n",
    "]\n",
    "\n",
    "Y_words[0] = np.array([tag2idx[t] for t in list(itertools.chain.from_iterable(Y_words[0]))])\n",
    "Y_words[1] = np.array([tag2idx[t] for t in list(itertools.chain.from_iterable(Y_words[1]))])\n",
    "Y_words[2] = np.array([tag2idx[t] for t in list(itertools.chain.from_iterable(Y_words[2]))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Build the Label Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.1 s, sys: 62.1 ms, total: 13.1 s\n",
      "Wall time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from scipy.sparse import dok_matrix, vstack, csr_matrix\n",
    "\n",
    "def create_word_lf_mat(Xs, Ls, num_lfs):\n",
    "    \"\"\"\n",
    "    Create word-level LF matrix from LFs indexed by sentence/word\n",
    "    0 words X lfs\n",
    "    1 words X lfs\n",
    "    2 words X lfs\n",
    "    ...\n",
    "    \n",
    "    \"\"\"\n",
    "    Yws = []\n",
    "    for sent_i in range(len(Xs)):\n",
    "        ys = dok_matrix((len(Xs[sent_i].words), num_lfs))\n",
    "        for lf_i in range(num_lfs):\n",
    "            for word_i,y in Ls[sent_i][lf_i].items():\n",
    "                ys[word_i, lf_i] = y\n",
    "        Yws.append(ys)\n",
    "    return csr_matrix(vstack(Yws))\n",
    "\n",
    "L_words = [\n",
    "    create_word_lf_mat(X_sents[0], L_sents[0], len(lfs)),\n",
    "    create_word_lf_mat(X_sents[1], L_sents[1], len(lfs)),\n",
    "    create_word_lf_mat(X_sents[2], L_sents[2], len(lfs)),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Inspect Labeling Function Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage%</th>\n",
       "      <th>Overlaps%</th>\n",
       "      <th>Conflicts%</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>guidelines</th>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 2.0]</td>\n",
       "      <td>0.006085</td>\n",
       "      <td>0.004668</td>\n",
       "      <td>0.001426</td>\n",
       "      <td>704</td>\n",
       "      <td>678</td>\n",
       "      <td>26</td>\n",
       "      <td>0.963068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stopwords</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.282796</td>\n",
       "      <td>0.021583</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>32717</td>\n",
       "      <td>32649</td>\n",
       "      <td>68</td>\n",
       "      <td>0.997922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>punctuation</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.099489</td>\n",
       "      <td>0.004296</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>11510</td>\n",
       "      <td>11425</td>\n",
       "      <td>85</td>\n",
       "      <td>0.992615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numbers</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.035387</td>\n",
       "      <td>0.002697</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>4094</td>\n",
       "      <td>3790</td>\n",
       "      <td>304</td>\n",
       "      <td>0.925745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UMLS_CHV</th>\n",
       "      <td>4</td>\n",
       "      <td>[1.0, 2.0]</td>\n",
       "      <td>0.352145</td>\n",
       "      <td>0.339499</td>\n",
       "      <td>0.016293</td>\n",
       "      <td>40740</td>\n",
       "      <td>39696</td>\n",
       "      <td>1044</td>\n",
       "      <td>0.974374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UMLS_SNOMEDCT_US</th>\n",
       "      <td>5</td>\n",
       "      <td>[1.0, 2.0]</td>\n",
       "      <td>0.334633</td>\n",
       "      <td>0.329516</td>\n",
       "      <td>0.017011</td>\n",
       "      <td>38714</td>\n",
       "      <td>37829</td>\n",
       "      <td>885</td>\n",
       "      <td>0.977140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UMLS_NCI</th>\n",
       "      <td>6</td>\n",
       "      <td>[1.0, 2.0]</td>\n",
       "      <td>0.397032</td>\n",
       "      <td>0.351419</td>\n",
       "      <td>0.018956</td>\n",
       "      <td>45933</td>\n",
       "      <td>45115</td>\n",
       "      <td>818</td>\n",
       "      <td>0.982191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UMLS_MSH</th>\n",
       "      <td>7</td>\n",
       "      <td>[1.0, 2.0]</td>\n",
       "      <td>0.181172</td>\n",
       "      <td>0.178622</td>\n",
       "      <td>0.010381</td>\n",
       "      <td>20960</td>\n",
       "      <td>20427</td>\n",
       "      <td>533</td>\n",
       "      <td>0.974571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UMLS_schwartz_hearst_1</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006033</td>\n",
       "      <td>0.005368</td>\n",
       "      <td>0.003293</td>\n",
       "      <td>698</td>\n",
       "      <td>649</td>\n",
       "      <td>49</td>\n",
       "      <td>0.929799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UMLS_schwartz_hearst_2</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.013130</td>\n",
       "      <td>0.011989</td>\n",
       "      <td>0.004685</td>\n",
       "      <td>1519</td>\n",
       "      <td>1207</td>\n",
       "      <td>312</td>\n",
       "      <td>0.794602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chebi</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.063730</td>\n",
       "      <td>0.055475</td>\n",
       "      <td>0.019958</td>\n",
       "      <td>7373</td>\n",
       "      <td>5242</td>\n",
       "      <td>2131</td>\n",
       "      <td>0.710972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         j    Polarity  Coverage%  Overlaps%  Conflicts%  \\\n",
       "guidelines               0  [1.0, 2.0]   0.006085   0.004668    0.001426   \n",
       "stopwords                1           2   0.282796   0.021583    0.000795   \n",
       "punctuation              2           2   0.099489   0.004296    0.000277   \n",
       "numbers                  3           2   0.035387   0.002697    0.001634   \n",
       "UMLS_CHV                 4  [1.0, 2.0]   0.352145   0.339499    0.016293   \n",
       "UMLS_SNOMEDCT_US         5  [1.0, 2.0]   0.334633   0.329516    0.017011   \n",
       "UMLS_NCI                 6  [1.0, 2.0]   0.397032   0.351419    0.018956   \n",
       "UMLS_MSH                 7  [1.0, 2.0]   0.181172   0.178622    0.010381   \n",
       "UMLS_schwartz_hearst_1   8           1   0.006033   0.005368    0.003293   \n",
       "UMLS_schwartz_hearst_2   9           2   0.013130   0.011989    0.004685   \n",
       "chebi                   10           1   0.063730   0.055475    0.019958   \n",
       "\n",
       "                        Coverage  Correct  Incorrect  Emp. Acc.  \n",
       "guidelines                   704      678         26   0.963068  \n",
       "stopwords                  32717    32649         68   0.997922  \n",
       "punctuation                11510    11425         85   0.992615  \n",
       "numbers                     4094     3790        304   0.925745  \n",
       "UMLS_CHV                   40740    39696       1044   0.974374  \n",
       "UMLS_SNOMEDCT_US           38714    37829        885   0.977140  \n",
       "UMLS_NCI                   45933    45115        818   0.982191  \n",
       "UMLS_MSH                   20960    20427        533   0.974571  \n",
       "UMLS_schwartz_hearst_1       698      649         49   0.929799  \n",
       "UMLS_schwartz_hearst_2      1519     1207        312   0.794602  \n",
       "chebi                       7373     5242       2131   0.710972  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trove.metrics.analysis import lf_summary\n",
    "\n",
    "lf_summary(L_words[0], Y=Y_words[0], lf_names=[lf.name for lf in lfs])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the Label Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trove uses a different internal mapping for labeling function abstains\n",
    "def convert_label_matrix(L):\n",
    "    # abstain is -1\n",
    "    # negative is 0\n",
    "    L = L.toarray().copy()\n",
    "    L[L == 0] = -1\n",
    "    L[L == 2] = 0\n",
    "    return L\n",
    "\n",
    "L_words_hat = [\n",
    "    convert_label_matrix(L_words[0]),\n",
    "    convert_label_matrix(L_words[1]),\n",
    "    convert_label_matrix(L_words[2])\n",
    "]\n",
    "\n",
    "Y_words_hat = [\n",
    "    np.array([0 if y == 2 else 1 for y in Y_words[0]]),\n",
    "    np.array([0 if y == 2 else 1 for y in Y_words[1]]),\n",
    "    np.array([0 if y == 2 else 1 for y in Y_words[2]])\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparamater Search Space: 192\n",
      "Using SEQUENCE dev checkpointing\n",
      "Using IO dev checkpointing\n",
      "Grid search over 25 configs\n",
      "[0] Label Model\n",
      "[1] Label Model\n",
      "[2] Label Model\n",
      "[3] Label Model\n",
      "[4] Label Model\n",
      "{'lr': 0.005, 'l2': 0.0001, 'n_epochs': 50, 'prec_init': 0.8, 'optimizer': 'adamax', 'lr_scheduler': 'constant'}\n",
      "[TRAIN] accuracy: 97.40 | precision: 77.40 | recall: 82.00 | f1: 79.63\n",
      "[DEV]   accuracy: 97.73 | precision: 80.34 | recall: 84.84 | f1: 82.53\n",
      "----------------------------------------------------------------------------------------\n",
      "[5] Label Model\n",
      "{'lr': 0.005, 'l2': 0.001, 'n_epochs': 50, 'prec_init': 0.6, 'optimizer': 'adamax', 'lr_scheduler': 'constant'}\n",
      "[TRAIN] accuracy: 97.65 | precision: 82.51 | recall: 81.35 | f1: 81.92\n",
      "[DEV]   accuracy: 98.00 | precision: 85.52 | recall: 84.24 | f1: 84.87\n",
      "----------------------------------------------------------------------------------------\n",
      "[6] Label Model\n",
      "[7] Label Model\n",
      "[8] Label Model\n",
      "[9] Label Model\n",
      "[10] Label Model\n",
      "[11] Label Model\n",
      "{'lr': 0.001, 'l2': 0.001, 'n_epochs': 100, 'prec_init': 0.6, 'optimizer': 'adamax', 'lr_scheduler': 'constant'}\n",
      "[TRAIN] accuracy: 97.58 | precision: 86.23 | recall: 77.09 | f1: 81.40\n",
      "[DEV]   accuracy: 97.93 | precision: 88.47 | recall: 81.93 | f1: 85.07\n",
      "----------------------------------------------------------------------------------------\n",
      "[12] Label Model\n",
      "[13] Label Model\n",
      "[14] Label Model\n",
      "[15] Label Model\n",
      "[16] Label Model\n",
      "[17] Label Model\n",
      "[18] Label Model\n",
      "[19] Label Model\n",
      "[20] Label Model\n",
      "[21] Label Model\n",
      "[22] Label Model\n",
      "[23] Label Model\n",
      "[24] Label Model\n",
      "BEST\n",
      "{'lr': 0.001, 'l2': 0.001, 'n_epochs': 100, 'prec_init': 0.6, 'optimizer': 'adamax', 'lr_scheduler': 'constant'}\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "from trove.models.model_search import grid_search\n",
    "from snorkel.labeling.model.label_model import LabelModel\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "n = L_words_hat[0].shape[0]\n",
    "\n",
    "param_grid = {\n",
    "    'lr': [0.01, 0.005, 0.001, 0.0001],\n",
    "    'l2': [0.001, 0.0001],\n",
    "    'n_epochs': [50, 100, 200, 600, 700, 1000],\n",
    "    'prec_init': [0.6, 0.7, 0.8, 0.9],\n",
    "    'optimizer': [\"adamax\"], \n",
    "    'lr_scheduler': ['constant'],\n",
    "\n",
    "#     'seed': list(np.random.randint(0,10000, 400)),\n",
    "#     'mu_eps': [1 / 10 ** np.ceil(np.log10(n*100)), \n",
    "#             1 / 10 ** np.ceil(np.log10(n*10)),\n",
    "#             1 / 10 ** np.ceil(np.log10(n))]\n",
    "}\n",
    "\n",
    "model_class_init = {\n",
    "    'cardinality': 2, \n",
    "    'verbose': True\n",
    "}\n",
    "\n",
    "n_model_search = 25\n",
    "num_hyperparams = functools.reduce(lambda x,y:x*y, [len(x) for x in param_grid.values()])\n",
    "print(\"Hyperparamater Search Space:\", num_hyperparams)\n",
    "\n",
    "\n",
    "L_train      = L_words_hat[0]\n",
    "Y_train      = Y_words_hat[0]\n",
    "L_dev        = L_words_hat[1]\n",
    "Y_dev        = Y_words_hat[1]\n",
    "\n",
    "label_model, best_config = grid_search(LabelModel, \n",
    "                                       model_class_init, \n",
    "                                       param_grid,\n",
    "                                       train = (L_train, Y_train, X_seq_lens[0]),\n",
    "                                       dev = (L_dev, Y_dev, X_seq_lens[1]),\n",
    "                                       n_model_search=n_model_search, \n",
    "                                       val_metric='f1', \n",
    "                                       seq_eval=True,\n",
    "                                       seed=1234,\n",
    "                                       tag_fmt_ckpnt='IO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIO Tag Format\n",
      "[Label Model]   accuracy: 97.58 | precision: 86.23 | recall: 77.09 | f1: 81.40\n",
      "[Majority Vote] accuracy: 97.54 | precision: 76.44 | recall: 80.77 | f1: 78.55\n",
      "--------------------------------------------------------------------------------\n",
      "BIO Tag Format\n",
      "[Label Model]   accuracy: 97.93 | precision: 88.47 | recall: 81.93 | f1: 85.07\n",
      "[Majority Vote] accuracy: 97.82 | precision: 78.67 | recall: 83.41 | f1: 80.97\n",
      "--------------------------------------------------------------------------------\n",
      "BIO Tag Format\n",
      "[Label Model]   accuracy: 98.10 | precision: 88.11 | recall: 80.96 | f1: 84.39\n",
      "[Majority Vote] accuracy: 97.79 | precision: 75.81 | recall: 82.01 | f1: 78.79\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from trove.metrics import eval_label_model # get_coverage, \n",
    "\n",
    "for i in range(3):\n",
    "    #get_coverage(L_words_hat[i], Y_words_hat[i])\n",
    "    #print(\"IO Tag Format\")\n",
    "    #eval_label_model(label_model, L_words_hat[i], Y_words_hat[i], X_seq_lens[i])\n",
    "    print(\"BIO Tag Format\")\n",
    "    eval_label_model(label_model, L_words_hat[i], Y_words_hat[i], X_seq_lens[i])\n",
    "    print('-' * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Proba Conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# TBD\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trove",
   "language": "python",
   "name": "trove"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
